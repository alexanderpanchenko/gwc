

%
%\begin{frame}
%  \frametitle{In close collaboration with ... }
%
% \includegraphics[width=.95\textwidth]{figures/collaborators}	
%\end{frame}


%
%\begin{frame}
%  \frametitle{In collaboration with ... }
%  { \large \bf
%  \begin{itemize}
%  	\item Andrei Kutuzov
%  	\item Eugen Ruppert
%  	\item Fide Marten
%  	\item Nikolay Arefyev
%  	\item Steffen Remus
%  	\item Martin Riedl
%  	\item Hubert Naets
%   	\item Maria Pelevina
%	\item Anastasiya Lopukhina
%	\item Konstantin Lopukhin
%  
%  \end{itemize}	
%  }
%\end{frame}


\section{Motivation}





\begin{frame}{Levels of Linguistic Analysis}
	\vspace{-15pt}
	
  \begin{center}
  	\includegraphics[width=0.5\textwidth]{figures/levels}
  \end{center}
   
{  \tiny
  Image source: \url{https://commons.wikimedia.org/wiki/File:Major_levels_of_linguistic_structure.svg}
 } 

\end{frame}



\begin{frame}{Levels of Linguistic Analysis}
	\vspace{-15pt}
	
  \begin{center}
  	\includegraphics[width=0.5\textwidth]{figures/levels2}
  \end{center}
   
{  \tiny
  Image source: \url{https://commons.wikimedia.org/wiki/File:Major_levels_of_linguistic_structure.svg}
 } 

\end{frame}




\begin{frame}{Linguistic Structures and Graphs}
	
	\begin{itemize}
		\item (Written) language is a \alert{symbolic system}
		\item \textbf{Semantic level}: typed weighted graphs of concepts
		\begin{itemize}
				\item Co-occurrence networks
 
		\item Lexical databases, e.g. WordNet
		\item Thesauri, e.g. NLM
		\item Ontologies, e.g. DBPedia
		\item Associative networks, e.g.  Edinburgh Associative Thesaurus
		\item ...
		
		\end{itemize}
	\end{itemize}	
	
\end{frame}





\begin{frame}{Semantic Graphs}
	
	\begin{center}
  	\includegraphics[width=.99\textwidth]{figures/graph1}
  \end{center}	
\end{frame}




\begin{frame}{Semantic Graphs}
	
	\begin{center}
  	\includegraphics[width=.99\textwidth]{figures/graph2}
  \end{center}	
\end{frame}


\begin{frame}{The new brave world of Deep Learning}
	\vspace{-15pt}
	
	
\begin{columns}
\begin{column}{0.6\textwidth}
   
 \begin{center}
  	\includegraphics[width=1.0\textwidth]{figures/backprop}
  \end{center}
  
\end{column}
\begin{column}{0.4\textwidth} 
  
 \begin{itemize}
 \item ''Anti-connectivism'' 
 \item End-to-end learning: \alert{symbolic representations aren't needed}
 \pause 
 
 \item Word embeddings lookup (at most)
 	
 \end{itemize}

  
\end{column}
\end{columns}

\end{frame}




\begin{frame}{Graph Matrix Duality}
	\vspace{-25pt}
	
  \begin{center}
  	\includegraphics[width=0.99\textwidth]{figures/graph2matrix}
  \end{center}
  
  \pause 

  \begin{itemize}
  	\item   Adjacency matrix $\mathbf{A}$ is dual with the corresponding graph $G$.
  	\pause 
  	\item Vector matrix multiply $\mathbf{A}^T\mathbf{x}$ is dual with breadth-first search.
  \end{itemize}
% 
%{  \footnotesize
%  Image source: \cite{kepner2011graph}
% } 
%
\end{frame}


\begin{frame}{Goal: Linguistic Structures in DL}

  \begin{enumerate}
  	\item \textbf{Learn interpretable symbolic structures from text} in an unsupervised way, which are \alert{more complex than words}.
  	\pause 
  	\item \textbf{Represent the learned structures} in a vector space.
  	\pause 
  	\item \textbf{Use the vector representations} instead/in addition to word embedding the deep learning applications. \alert{Lookup of word senses, frames, etc.}
  	\pause 
  	\item More complex structures could improve performance, but also provide \textbf{better interpretability of the deep learning models}. 
  	
  \end{enumerate}

\end{frame}
